{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cbadddb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-25T13:20:15.206703Z",
     "iopub.status.busy": "2022-01-25T13:20:15.205761Z",
     "iopub.status.idle": "2022-01-25T13:20:15.218053Z",
     "shell.execute_reply": "2022-01-25T13:20:15.218981Z",
     "shell.execute_reply.started": "2022-01-24T14:12:59.721735Z"
    },
    "papermill": {
     "duration": 0.0354,
     "end_time": "2022-01-25T13:20:15.219863",
     "exception": false,
     "start_time": "2022-01-25T13:20:15.184463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IT IS LIGHT NORMALAZING! IT IS WITHOUT NTLK,TEXTBLOOB OR SPACY\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from time import time \n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "num_words=300 #How many words we will use for modeling\n",
    "\n",
    "#STOP_WORDS.Here is minus_words. This is list of words which can't help us in creating our model \n",
    "#Some ntlk stop words + my words. All words are without \" ' \".\n",
    "stop_words=['inside','ever','instead','already','theres','anything','thats','always','could','hers', 'only', 'neednt', 'doesnt', 'without', 'have', 'its', 'don', 'ive', 'into', 'before', 'mustn', 'any', 'now', 'such', 'you', 'trump', 'below', 'since', 'which', 'doesn', 'hasn', 'itself', 'all', 'someone', 'couldn', 'some', 'down', 'under', 'her', 'weren', 'once', 'yourselves', 'youd', 'myself', 'didnt', 'thi', 'the', 'dont', 'but', 'their', 'werent', 'yourself', 'over', 'mightnt', 'hes', 'too', 'each', 'yal', 'when', 'that', 'hey', 'everything', 'with', 'out', 'who', 'both', 'few', 'else', 'than', 'mightn', 'own', 'haven', 'everyone', 'then', 'our', 'was', 'themselves', 'isnt', 'gues', 'arent', 'where', 'youll', 'how', 'again', 'hasnt', 'another', 'why', 'more', 'hadnt', 'shant', 'doing', 'having', 'while', 'were', 'youve', 'been', 'she', 'and', 'there', 'needn', 'hadn', 'wasnt', 'against', 'him', 'wont', 'cant', 'these', 'wasn', 'himself', 'theirs', 'same', 'because', 'wouldn', 'your', 'whom', 'until', 'does', 'here', 'video', 'aren', 'just', 'thatll', 'mustnt', 'nor', 'shouldn', 'wouldnt','would', 'will', 'them', 'above', 'his', 'youre', 'couldnt', 'has', 'wil', 'shan', 'isn', 'herself', 'ain', 'anyone', 'this', 'about', 'after', 'shouldve', 'further', 'every', 'from', 'what', 'shes', 'shouldnt', 'during', 'something', 'between', 'other', 'yours', 'though', 'ours', 'had', 'also', 'should', 'for', 'those', 'they', 'not', 'most', 'off', 'through', 'being', 'did', 'didn', 'can', 'ourselves', 'havent', 'won', 'very', 'are']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0785b7eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T13:20:15.242326Z",
     "iopub.status.busy": "2022-01-25T13:20:15.241585Z",
     "iopub.status.idle": "2022-01-25T13:20:21.485408Z",
     "shell.execute_reply": "2022-01-25T13:20:21.484738Z",
     "shell.execute_reply.started": "2022-01-24T14:13:06.128485Z"
    },
    "papermill": {
     "duration": 6.258071,
     "end_time": "2022-01-25T13:20:21.485611",
     "exception": false,
     "start_time": "2022-01-25T13:20:15.227540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (2,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#Creating DataFrames\n",
    "df_comments_us=pd.read_csv('/kaggle/input/youtube/UScomments.csv',on_bad_lines='skip')\n",
    "df_comments_gb=pd.read_csv('/kaggle/input/youtube/GBcomments.csv',on_bad_lines='skip')\n",
    "df_videos_us=pd.read_csv('/kaggle/input/youtube/USvideos.csv',on_bad_lines='skip')\n",
    "df_videos_gb=pd.read_csv('/kaggle/input/youtube/GBvideos.csv',on_bad_lines='skip')\n",
    "\n",
    "#Connecting GB and US DataFrames\n",
    "df_videos=pd.concat([df_videos_us, df_videos_gb], ignore_index=True)\n",
    "df_comments=pd.concat([df_comments_us, df_comments_gb], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4465cc3",
   "metadata": {
    "papermill": {
     "duration": 0.007298,
     "end_time": "2022-01-25T13:20:21.501783",
     "exception": false,
     "start_time": "2022-01-25T13:20:21.494485",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f33547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T13:20:21.534382Z",
     "iopub.status.busy": "2022-01-25T13:20:21.533644Z",
     "iopub.status.idle": "2022-01-25T13:20:21.567254Z",
     "shell.execute_reply": "2022-01-25T13:20:21.566561Z",
     "shell.execute_reply.started": "2022-01-24T14:13:10.973479Z"
    },
    "papermill": {
     "duration": 0.05709,
     "end_time": "2022-01-25T13:20:21.567456",
     "exception": false,
     "start_time": "2022-01-25T13:20:21.510366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          video_id   likes  dislikes  preferences\n",
      "0      XpVt6Z1Gjjo  320053      5931     0.981806\n",
      "2      cLdxuaxaQwc  576597     39774     0.935471\n",
      "3      WYYvHb03Eog   24975      4542     0.846123\n",
      "4      sjlHnJvXdQs   96666       568     0.994158\n",
      "5      cMKX2tE5Luk   34507       544     0.984480\n",
      "...            ...     ...       ...          ...\n",
      "15979  qnSp6GwsI78   18405       491     0.974016\n",
      "15980  T2RUYYs8Hxc    1846        92     0.952528\n",
      "15982  1zOPtQNChZM    6592       127     0.981098\n",
      "15983  Z7R8XRKqHAI   44811       652     0.985659\n",
      "15984  lLN1FwiqGwc   58532      3223     0.947810\n",
      "\n",
      "[13108 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Creating new DataFrame for normalize data\n",
    "new_df=pd.DataFrame()\n",
    "\n",
    "#New DF with videos that have 50+ comments and 1000+ likes. I think less isn't indicative\n",
    "new_df[['video_id','likes','dislikes']]=df_videos[df_videos['comment_total']>49][df_videos['likes']>999][['video_id','likes','dislikes']]\n",
    "\n",
    "#New column with likes/dislikes ratio \n",
    "new_df['preferences'] = new_df['likes']/(new_df['dislikes']+new_df['likes'])\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce26ac6",
   "metadata": {
    "papermill": {
     "duration": 0.008231,
     "end_time": "2022-01-25T13:20:21.585112",
     "exception": false,
     "start_time": "2022-01-25T13:20:21.576881",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cffad44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T13:20:21.633289Z",
     "iopub.status.busy": "2022-01-25T13:20:21.632219Z",
     "iopub.status.idle": "2022-01-25T13:20:21.639256Z",
     "shell.execute_reply": "2022-01-25T13:20:21.639883Z",
     "shell.execute_reply.started": "2022-01-24T14:13:11.015816Z"
    },
    "papermill": {
     "duration": 0.043926,
     "end_time": "2022-01-25T13:20:21.640110",
     "exception": false,
     "start_time": "2022-01-25T13:20:21.596184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median is 0.97677. It means 50% of videos have likes ratio above/below median\n",
      "          video_id   likes  dislikes  preferences  prefers_binary\n",
      "0      XpVt6Z1Gjjo  320053      5931     0.981806               1\n",
      "2      cLdxuaxaQwc  576597     39774     0.935471               0\n",
      "3      WYYvHb03Eog   24975      4542     0.846123               0\n",
      "4      sjlHnJvXdQs   96666       568     0.994158               1\n",
      "5      cMKX2tE5Luk   34507       544     0.984480               1\n",
      "...            ...     ...       ...          ...             ...\n",
      "15979  qnSp6GwsI78   18405       491     0.974016               0\n",
      "15980  T2RUYYs8Hxc    1846        92     0.952528               0\n",
      "15982  1zOPtQNChZM    6592       127     0.981098               1\n",
      "15983  Z7R8XRKqHAI   44811       652     0.985659               1\n",
      "15984  lLN1FwiqGwc   58532      3223     0.947810               0\n",
      "\n",
      "[13108 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#I'd like to create model wich can predict likes ratio \n",
    "#But I feel that this dataset is not enough :(\n",
    "#So my another one model will predict video in top 50% or not \n",
    "#So I need to find median and create binary classification for videos \n",
    "\n",
    "#Find median\n",
    "median=new_df['preferences'].median() \n",
    "print(f'Median is {round(median,5)}. It means 50% of videos have likes ratio above/below median')\n",
    "\n",
    "#Create new column with binray preferences \n",
    "new_df['prefers_binary']=new_df['preferences'].apply(lambda b:1 if b>=median else 0)\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e892c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T13:20:21.672618Z",
     "iopub.status.busy": "2022-01-25T13:20:21.663567Z",
     "iopub.status.idle": "2022-01-25T13:20:21.676268Z",
     "shell.execute_reply": "2022-01-25T13:20:21.675699Z",
     "shell.execute_reply.started": "2022-01-24T14:14:07.434871Z"
    },
    "papermill": {
     "duration": 0.026081,
     "end_time": "2022-01-25T13:20:21.676405",
     "exception": false,
     "start_time": "2022-01-25T13:20:21.650324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now we can prepare text for normalization \n",
    "#I'll find all words in comments in each video, fix words and count them\n",
    "#And in this cell all functions for that\n",
    "#Some functions are active just in this ver (remove_duplicates_in_words,delete_s)\n",
    "#Instead of them in full ver I will be use leming or steming\n",
    "\n",
    "\n",
    "#Here we lost duplicates also in right words but I hope it isn't destroy our model \n",
    "def remove_duplicates_in_words(word): #Dooooon't tyyyype coooommmmments liiike thaaaaat =>Don't type coments like that\n",
    "    l = ''\n",
    "    new_word = ''\n",
    "    for i in word:\n",
    "        if i != l:\n",
    "            new_word += i\n",
    "        l = i\n",
    "    return new_word\n",
    "\n",
    "\n",
    "#Remove s or ed on end of words\n",
    "def delete_s_ed(word):\n",
    "    if word[-1:]=='s':\n",
    "        new_word=word[:-1]\n",
    "    elif word[-2:]=='ed':\n",
    "        new_word=word[:-2]\n",
    "    else:\n",
    "        new_word=word\n",
    "    return new_word\n",
    "\n",
    "\n",
    "#Only text and text to list of words\n",
    "def cleaning_text(text): \n",
    "    try:\n",
    "        text=text.lower()\n",
    "        text=''.join([x for x in text if x in string.ascii_letters + ' ']) #just letters and sapces\n",
    "    except: #Sometimes no words in comments. People like smiles (:\n",
    "        text=\" \"\n",
    "    return text.split(\" \")\n",
    "\n",
    "\n",
    "#This function remove useless words and fix useful if it needs\n",
    "#It is light ver so here are not leming or steming\n",
    "def words_cleaning(words): \n",
    "    new_words=[]   \n",
    "    for word in words:\n",
    "        word=remove_duplicates_in_words(word) \n",
    "        if len(word)<13 and len(word)>=3: # In comments words with length more than 12 are rare and less than 3 are useless\n",
    "                if word not in stop_words: #Is word in minus(stop) words?\n",
    "                    new_words.append(delete_s_ed(word))       \n",
    "    return new_words\n",
    "\n",
    "\n",
    "#From list of words create dict={word:frequencies} . \n",
    "def text_to_frequencies(text): \n",
    "    words=set(text)\n",
    "    frequencies={}\n",
    "    for word in words:\n",
    "        frequency=text.count(word)\n",
    "        frequencies.update({word:frequency})\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50adf01e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T13:20:21.817731Z",
     "iopub.status.busy": "2022-01-25T13:20:21.704240Z",
     "iopub.status.idle": "2022-01-25T14:08:24.437525Z",
     "shell.execute_reply": "2022-01-25T14:08:24.436744Z",
     "shell.execute_reply.started": "2022-01-24T14:14:10.652999Z"
    },
    "papermill": {
     "duration": 2882.752587,
     "end_time": "2022-01-25T14:08:24.437700",
     "exception": false,
     "start_time": "2022-01-25T13:20:21.685113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working, there are still 12608 interation of 13108\n",
      "Finish in 44 min.\n",
      "Working, there are still 12108 interation of 13108\n",
      "Finish in 43 min.\n",
      "Working, there are still 11608 interation of 13108\n",
      "Finish in 40 min.\n",
      "Working, there are still 11108 interation of 13108\n",
      "Finish in 38 min.\n",
      "Working, there are still 10608 interation of 13108\n",
      "Finish in 36 min.\n",
      "Working, there are still 10108 interation of 13108\n",
      "Finish in 34 min.\n",
      "Working, there are still 9608 interation of 13108\n",
      "Finish in 33 min.\n",
      "Working, there are still 9108 interation of 13108\n",
      "Finish in 31 min.\n",
      "Working, there are still 8608 interation of 13108\n",
      "Finish in 30 min.\n",
      "Working, there are still 8108 interation of 13108\n",
      "Finish in 28 min.\n",
      "Working, there are still 7608 interation of 13108\n",
      "Finish in 26 min.\n",
      "Working, there are still 7108 interation of 13108\n",
      "Finish in 25 min.\n",
      "Working, there are still 6608 interation of 13108\n",
      "Finish in 22 min.\n",
      "Working, there are still 6108 interation of 13108\n",
      "Finish in 21 min.\n",
      "Working, there are still 5608 interation of 13108\n",
      "Finish in 19 min.\n",
      "Working, there are still 5108 interation of 13108\n",
      "Finish in 18 min.\n",
      "Working, there are still 4608 interation of 13108\n",
      "Finish in 16 min.\n",
      "Working, there are still 4108 interation of 13108\n",
      "Finish in 14 min.\n",
      "Working, there are still 3608 interation of 13108\n",
      "Finish in 13 min.\n",
      "Working, there are still 3108 interation of 13108\n",
      "Finish in 11 min.\n",
      "Working, there are still 2608 interation of 13108\n",
      "Finish in 9 min.\n",
      "Working, there are still 2108 interation of 13108\n",
      "Finish in 8 min.\n",
      "Working, there are still 1608 interation of 13108\n",
      "Finish in 6 min.\n",
      "Working, there are still 1108 interation of 13108\n",
      "Finish in 4 min.\n",
      "Working, there are still 608 interation of 13108\n",
      "Finish in 2 min.\n",
      "Working, there are still 108 interation of 13108\n",
      "Finish in 0 min.\n"
     ]
    }
   ],
   "source": [
    "#Creating new column with lists of words\n",
    "n=0\n",
    "start_time=time()#timer\n",
    "\n",
    "df_len=len(new_df)\n",
    "all_comments_words=[]\n",
    "\n",
    "#Finding comments for each video\n",
    "for video in new_df['video_id'].values: \n",
    "    comments_words=[]\n",
    "    \n",
    "    #timer\n",
    "    n+=1\n",
    "    if n%500==0:\n",
    "        print(f\"Working, there are still {df_len-n} interation of {df_len}\\nFinish in {round((df_len-n)/60*((time()-start_time)/n))} min.\")\n",
    "    #timer\n",
    "    \n",
    "    for comment in df_comments[df_comments.video_id==video]['comment_text'].values:\n",
    "        words=words_cleaning(cleaning_text(comment)) \n",
    "        comments_words+=words\n",
    "    comments_words=text_to_frequencies(comments_words)\n",
    "    all_comments_words.append(comments_words)\n",
    "\n",
    "#New column with dict:{word:frequencies} \n",
    "new_df['words']=all_comments_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5e846",
   "metadata": {
    "papermill": {
     "duration": 0.013636,
     "end_time": "2022-01-25T14:08:24.465485",
     "exception": false,
     "start_time": "2022-01-25T14:08:24.451849",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae5524cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T14:08:24.514291Z",
     "iopub.status.busy": "2022-01-25T14:08:24.508896Z",
     "iopub.status.idle": "2022-01-25T14:08:45.191464Z",
     "shell.execute_reply": "2022-01-25T14:08:45.190903Z",
     "shell.execute_reply.started": "2022-01-24T14:13:11.603111Z"
    },
    "papermill": {
     "duration": 20.712482,
     "end_time": "2022-01-25T14:08:45.191675",
     "exception": false,
     "start_time": "2022-01-25T14:08:24.479193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "#Find most popular words, create columns for each populae word and add them frequencies in cells\n",
    "big_dict=Counter() #Dictionary for all words\n",
    "\n",
    "#Summary all dicts from df.words\n",
    "for words in new_df['words'].values:\n",
    "    big_dict.update(words)\n",
    "\n",
    "#Most popular num_words. Class Counter to dict, dict to items, sort it, generate list of words and slice\n",
    "new_columns=[k[0] for k in sorted(dict(big_dict).items(), key=lambda item: item[1],reverse=True)][:num_words]\n",
    "\n",
    "#Creating new columns of most popular words with absolute values of frequencies\n",
    "for column in new_columns: \n",
    "    columns_values=[]\n",
    "    for words in new_df['words'].values:\n",
    "        try:\n",
    "            q=words[column]\n",
    "        except:\n",
    "            q=0\n",
    "        columns_values.append(q)\n",
    "    new_df[column]=columns_values\n",
    "    \n",
    "#Save data. Save time for next time \n",
    "new_df.to_csv(f'./data{num_words}_light.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2924.577574,
   "end_time": "2022-01-25T14:08:47.128042",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-25T13:20:02.550468",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
